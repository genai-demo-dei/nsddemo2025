{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ9RuteB5G7Y"
      },
      "source": [
        "Login into https://aistudio.google.com/app/apikey and click get api key. This will generate the API key for Google Gemini Flash 2.0 lite. Keep the key secure and treat it as confidential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG_xvBmm5pje"
      },
      "source": [
        "Making your first API call to Gemini model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQeVl9ou57vO",
        "outputId": "c6ca473d-ed9f-442e-b9f0-279e19b1846a"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Replace with your actual API key\n",
        "api_key = \"<Enter Key>\"\n",
        "\n",
        "# API endpoint URL\n",
        "url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=\" + api_key\n",
        "\n",
        "# Request headers\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Request data (prompt)\n",
        "data = {\n",
        "    \"contents\": [\n",
        "        {\n",
        "            \"parts\": [\n",
        "                {\"text\": \"Explain how AI works\"}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "try:\n",
        "  # Send POST request to Gemini API\n",
        "  response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "  # Check for successful request\n",
        "  response.raise_for_status()\n",
        "\n",
        "  # Process the JSON response\n",
        "  response_json = response.json()\n",
        "\n",
        "  # Extract and print the generated text\n",
        "  generated_text = response_json.get('candidates', [{}])[0].get('content', '')\n",
        "  print(generated_text)\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "  print(f\"An error occurred: {e}\")\n",
        "except (KeyError, IndexError) as e:\n",
        "  print(f\"Error parsing response: {e}\")\n",
        "  print(f\"Full response: {response.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xhzC6BVcAWru",
        "outputId": "69071530-b001-4cdf-ac76-fcb1785f3f0d"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber\n",
        "!pip install google-generativeai\n",
        "!pip install langchain\n",
        "!pip install faiss-cpu\n",
        "!pip install -U langchain-google-genai\n",
        "!pip install -U langchain-community\n",
        "\n",
        "import pdfplumber\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings # Import GoogleGenerativeAIEmbeddings from langchain_google_genai\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMhSYZVr-TsS"
      },
      "source": [
        "### **Installing Dependencies:**\n",
        "pdfplumber: Extracts text from PDF files.\n",
        "google-generativeai: Provides access to Google's Generative AI models.\n",
        "langchain: A framework for working with LLMs (large language models).\n",
        "faiss-cpu: A library for efficient similarity search and clustering.\n",
        "langchain-google-genai: Integration of LangChain with Google Generative AI.\n",
        "langchain-community: An updated community-driven LangChain package.\n",
        "Importing Required Libraries:\n",
        "\n",
        "**pdfplumber:** Reads and extracts text from PDF documents.\n",
        "**google.generativeai:** Connects to Google's Generative AI models.\n",
        "**GoogleGenerativeAIEmbeddings:** Generates vector embeddings from text using Google's AI.\n",
        "**FAISS:** A vector store for storing and retrieving similar text chunks.\n",
        "**TextLoader:** Handles loading text-based documents (not used in this script).\n",
        "**RecursiveCharacterTextSplitter:** Splits text into smaller chunks for better processing.\n",
        "**Document: **Represents a structured text document.\n",
        "**GoogleGenerativeAI:** Utilizes Google's AI model for generating responses.\n",
        "**os:** Used for setting environment variables (like API keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGawgyjM-17a"
      },
      "outputs": [],
      "source": [
        "# Set up Gemini API Key (replace with your actual API key)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"<ENTER KEY>\"\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a given PDF file using pdfplumber.\"\"\"\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\" if page.extract_text() else \"\"  # Avoid None values\n",
        "    return text.strip()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1bqEhh5A0f1"
      },
      "source": [
        "# **Key Functionalities**\n",
        "# **1. API Key Configuration**\n",
        "The Google Gemini API Key is set as an environment variable using os.environ[\"GOOGLE_API_KEY\"].\n",
        "This allows secure access to Google's Generative AI models for embedding generation and text-based responses.\n",
        "# **2. Extracting Text from PDFs**(extract_text_from_pdf)\n",
        "Uses pdfplumber to read and extract text from a given PDF file.\n",
        "Iterates through each page of the document and retrieves its text.\n",
        "Handles None values to prevent errors during processing.\n",
        "Returns the cleaned text as a single string.\n",
        "# 3.**Processing and Vectorizing** the Extracted Text (vectorize_pdf)\n",
        "Calls extract_text_from_pdf(pdf_path) to retrieve the text content of a PDF.\n",
        "Prepares the extracted text for further processing and vector embedding (though vectorization is not yet implemented in this snippet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m82yHHf-BRKW"
      },
      "outputs": [],
      "source": [
        "def vectorize_pdf(pdf_path):\n",
        "    \"\"\"Processes and vectorizes the text from a PDF file.\"\"\"\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Split text into smaller chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks = text_splitter.split_text(text)\n",
        "\n",
        "    # Convert chunks into Document objects\n",
        "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
        "\n",
        "    # Initialize Google Gemini Embeddings\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "    # Store in FAISS vector database\n",
        "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
        "    return vectorstore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs3XXT_WBkJE"
      },
      "source": [
        "# **Key Components**\n",
        "# **1. Splitting Text into Chunks**\n",
        "\n",
        "Why split the text? Large documents can be difficult to process, so breaking them into smaller segments ensures better search accuracy.\n",
        "RecursiveCharacterTextSplitter:\n",
        "chunk_size=500: Each text chunk will have approximately 500 characters.\n",
        "chunk_overlap=50: Ensures some overlap between consecutive chunks to maintain context.\n",
        "# 2. **Converting Chunks into Document Objects**\n",
        "\n",
        "Each chunk is wrapped inside a Document object, making it compatible with vectorization and retrieval models.\n",
        "# 3. **Initializing Google Gemini Embeddings**\n",
        "\n",
        "GoogleGenerativeAIEmbeddings converts text chunks into numerical vector representations.\n",
        "These vectors allow the system to perform similarity searches and retrieve relevant content.\n",
        "# 4. **Storing Vectors in FAISS**\n",
        "\n",
        "FAISS (Facebook AI Similarity Search) is an efficient indexing system for fast similarity searches.\n",
        "It stores document vectors, enabling quick and accurate retrieval when querying the document later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVFfX0RxBl3U"
      },
      "outputs": [],
      "source": [
        "def query_pdf(vectorstore, query):\n",
        "    \"\"\"Retrieves relevant information from the vectorstore and generates a response.\"\"\"\n",
        "    # Search for relevant documents\n",
        "    docs = vectorstore.similarity_search(query, k=3)\n",
        "    context = \"\\n\".join([doc.page_content for doc in docs]\n",
        "                        )\n",
        "\n",
        "    # Initialize Gemini Flash 2.0 Lite model\n",
        "    llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n",
        "    # Generate response based on context\n",
        "    prompt = f\"Using the following extracted information from a PDF, answer the user's question:\\n\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "    response = llm.invoke(prompt)\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DynlYMcCfSc"
      },
      "source": [
        "# **1. Retrieving Relevant Information**\n",
        "\n",
        "similarity_search(query, k=3): Searches the FAISS database for the top 3 most relevant text chunks related to the user’s query.\n",
        "Joins retrieved document chunks into a single string (context) to provide meaningful context for the LLM.\n",
        "# **2. Initializing the Google Gemini Model**\n",
        "\n",
        "Loads the Gemini 1.5 Flash model, a fast and efficient generative AI designed for real-time question answering.\n",
        "# **3. Creating the Prompt for AI Response Generation**\n",
        "\n",
        "Prompt Engineering:\n",
        "Provides retrieved context from the PDF.\n",
        "Clearly defines the user’s question to guide the AI model.\n",
        "Ensures the model stays factually grounded in the document content.\n",
        "# **4. Generating and Returning the Response**\n",
        "\n",
        "llm.invoke(prompt): Uses the AI model to generate an answer based on the context.\n",
        "Returns the AI-generated response to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b3a68KJCztT",
        "outputId": "ce3d2bac-66ef-4120-f3a8-f6570321b102"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "pdf_path = \"/content/weekly-report-7.pdf\"  # Provide the path to your PDF file\n",
        "vectorstore = vectorize_pdf(pdf_path)\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nAsk a question (or type 'exit' to quit): \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    answer = query_pdf(vectorstore, query)\n",
        "    print(\"\\nResponse:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxYt86D8DFob"
      },
      "source": [
        "# **Key Components**\n",
        "# **1. Defining the PDF Path and Vectorizing Its content**\n",
        "\n",
        "pdf_path: Specifies the location of the PDF file to be processed.\n",
        "vectorize_pdf(pdf_path): Extracts, chunks, and embeds the PDF text into the FAISS vector database for efficient retrieval.\n",
        "# **2. User Input Loop for Querying the PDF**\n",
        "\n",
        "Starts an infinite loop to continuously accept user questions.\n",
        "Allows users to type queries dynamically.\n",
        "Includes an exit condition to terminate the program when \"exit\" is entered.\n",
        "# **3. Querying the Vector Database and Generating Responses**\n",
        "\n",
        "Checks if the user input is \"exit\" (case insensitive) and breaks the loop if true.\n",
        "Calls query_pdf(vectorstore, query) to retrieve the most relevant text chunks and generate a response using the AI model.\n",
        "Prints the AI-generated response for the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWm9iClmDb-8"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
