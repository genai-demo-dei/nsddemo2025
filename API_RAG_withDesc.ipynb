{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ9RuteB5G7Y"
      },
      "source": [
        "Login into https://aistudio.google.com/app/apikey and click get api key. This will generate the API key for Google Gemini Flash 2.0 lite. Keep the key secure and treat it as confidential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG_xvBmm5pje"
      },
      "source": [
        "Making your first API call to Gemini model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQeVl9ou57vO",
        "outputId": "c6ca473d-ed9f-442e-b9f0-279e19b1846a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'parts': [{'text': 'Okay, let\\'s break down how AI works.  It\\'s a broad field, so I\\'ll give you a general overview and then dive into some key concepts and common techniques.\\n\\n**What is AI?**\\n\\nAt its core, Artificial Intelligence (AI) is about creating machines that can perform tasks that typically require human intelligence. These tasks can include:\\n\\n*   **Learning:** Acquiring information and rules for using it.\\n*   **Reasoning:** Using rules to reach conclusions.\\n*   **Problem-solving:** Discovering and implementing procedures for solving problems.\\n*   **Perception:** Gathering information through sensors (e.g., cameras, microphones) and interpreting it.\\n*   **Language understanding:** Understanding and generating natural language.\\n*   **Decision-making:** Choosing the best course of action.\\n\\n**The Basic Building Blocks**\\n\\nHere\\'s a simplified view of the fundamental components involved in most AI systems:\\n\\n1.  **Data:**  AI systems need data to learn from. This can be anything from images and text to sensor readings and financial records. The quality and quantity of data are crucial for the AI\\'s performance.\\n\\n2.  **Algorithms:** These are the sets of instructions that tell the AI how to process the data and perform a specific task.  Different algorithms are suited for different types of problems.\\n\\n3.  **Models:**  A model is a representation of what the AI has learned from the data. It\\'s essentially a mathematical function that maps inputs to outputs, based on the patterns discovered in the data.\\n\\n4.  **Training:**  This is the process of feeding data into the algorithm so that it can learn and build a model. During training, the AI adjusts its internal parameters to improve its performance on the given task.  This is often done through optimization techniques that minimize errors.\\n\\n5.  **Inference:** Once the model is trained, it can be used to make predictions or decisions on new, unseen data. This is called inference or prediction.\\n\\n**Key Concepts and Techniques**\\n\\nNow let\\'s look at some of the most common and important techniques used in AI:\\n\\n*   **Machine Learning (ML):** This is a subset of AI that focuses on enabling machines to learn from data without being explicitly programmed. Instead of writing specific rules, you provide the machine with data, and it figures out the rules on its own.\\n\\n    *   **Supervised Learning:** The AI is given labeled data, meaning the correct output is provided for each input. The goal is to learn a mapping from inputs to outputs. Examples include:\\n        *   *Classification:* Categorizing data into different classes (e.g., identifying whether an email is spam or not).\\n        *   *Regression:* Predicting a continuous value (e.g., predicting the price of a house based on its features).\\n        *   Common algorithms: Linear Regression, Logistic Regression, Support Vector Machines (SVMs), Decision Trees, Random Forests, Naive Bayes.\\n\\n    *   **Unsupervised Learning:** The AI is given unlabeled data, and it has to find patterns and structures on its own. Examples include:\\n        *   *Clustering:* Grouping similar data points together (e.g., segmenting customers into different groups based on their purchasing behavior).\\n        *   *Dimensionality Reduction:* Reducing the number of variables in a dataset while preserving its essential information (e.g., simplifying complex data for visualization).\\n        *   *Association Rule Mining:* Discovering relationships between items in a dataset (e.g., finding that customers who buy bread often also buy milk).\\n        *   Common algorithms: K-Means Clustering, Hierarchical Clustering, Principal Component Analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE), Apriori.\\n\\n    *   **Reinforcement Learning (RL):** The AI learns by interacting with an environment and receiving rewards or penalties for its actions. The goal is to learn a policy that maximizes the cumulative reward. Think of it like training a dog with treats. Examples include:\\n        *   *Game playing:* Training an AI to play games like chess or Go.\\n        *   *Robotics:* Training a robot to navigate a maze or perform a task.\\n        *   *Control systems:* Optimizing the control of a system, such as a self-driving car.\\n        *   Common algorithms: Q-Learning, Deep Q-Network (DQN), Proximal Policy Optimization (PPO).\\n\\n*   **Deep Learning (DL):** This is a subfield of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to analyze data. Deep learning models can automatically learn complex features from raw data, making them very powerful for tasks like image recognition, natural language processing, and speech recognition.\\n\\n    *   **Neural Networks:** Inspired by the structure of the human brain, neural networks consist of interconnected nodes (neurons) organized in layers. Each connection has a weight that determines the strength of the connection.\\n    *   **Convolutional Neural Networks (CNNs):**  Especially effective for image and video processing. They use convolutional layers to extract features from images.\\n    *   **Recurrent Neural Networks (RNNs):** Designed for processing sequential data, such as text and audio. They have feedback connections that allow them to remember past information. Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) are popular types of RNNs.\\n    *   **Transformers:**  A more recent architecture that has revolutionized natural language processing. Transformers use attention mechanisms to focus on the most relevant parts of the input sequence.\\n\\n*   **Natural Language Processing (NLP):**  This field focuses on enabling computers to understand, interpret, and generate human language.  It involves tasks like:\\n\\n    *   *Text classification:* Categorizing text into different categories (e.g., sentiment analysis).\\n    *   *Machine translation:* Translating text from one language to another.\\n    *   *Text summarization:* Generating a concise summary of a longer text.\\n    *   *Question answering:* Answering questions based on a given text.\\n    *   *Named entity recognition:* Identifying and classifying named entities in text (e.g., people, organizations, locations).\\n    *   *Chatbots:* Developing conversational AI systems that can interact with humans.\\n\\n*   **Computer Vision:**  This field focuses on enabling computers to \"see\" and interpret images and videos.  It involves tasks like:\\n\\n    *   *Image classification:* Identifying the objects in an image.\\n    *   *Object detection:* Locating and identifying multiple objects in an image.\\n    *   *Image segmentation:* Dividing an image into different regions based on their content.\\n    *   *Image generation:* Creating new images from scratch or modifying existing images.\\n\\n*   **Expert Systems:**  These are AI systems designed to mimic the decision-making abilities of a human expert in a specific domain. They typically use a knowledge base containing facts and rules to reason about the problem and provide advice or solutions.\\n\\n*   **Robotics:**  This field combines AI with engineering to create robots that can perform physical tasks.  AI is used to control the robot\\'s movements, perception, and decision-making.\\n\\n**The AI Development Process (Simplified)**\\n\\n1.  **Define the problem:** What specific task do you want the AI to perform?\\n2.  **Gather data:** Collect relevant data that the AI can learn from.\\n3.  **Prepare the data:** Clean, preprocess, and transform the data into a suitable format for the AI algorithm.\\n4.  **Choose an algorithm:** Select an algorithm that is appropriate for the problem and the data.\\n5.  **Train the model:** Train the algorithm on the data to build a model.\\n6.  **Evaluate the model:** Assess the performance of the model on a separate dataset.\\n7.  **Tune the model:** Adjust the model\\'s parameters to improve its performance.\\n8.  **Deploy the model:** Integrate the model into an application or system.\\n9.  **Monitor and maintain:** Continuously monitor the model\\'s performance and retrain it as needed to maintain its accuracy.\\n\\n**Examples of AI in Action**\\n\\n*   **Self-driving cars:** Use computer vision, sensor fusion, and machine learning to navigate roads.\\n*   **Spam filters:** Use machine learning to identify and filter out spam emails.\\n*   **Recommendation systems:** Use machine learning to recommend products or content to users.\\n*   **Voice assistants:** Use natural language processing and speech recognition to understand and respond to voice commands.\\n*   **Medical diagnosis:** Use machine learning to analyze medical images and data to assist in diagnosis.\\n*   **Fraud detection:** Use machine learning to identify fraudulent transactions.\\n\\n**Challenges and Limitations**\\n\\n*   **Data requirements:** AI models, especially deep learning models, often require massive amounts of data to train effectively.\\n*   **Bias:** AI models can inherit biases from the data they are trained on, leading to unfair or discriminatory outcomes.\\n*   **Explainability:** Some AI models, particularly deep learning models, are difficult to understand, making it hard to explain why they make certain decisions (the \"black box\" problem).\\n*   **Ethical considerations:** The use of AI raises ethical concerns related to privacy, fairness, accountability, and job displacement.\\n*   **Overfitting:**  A model can learn the training data *too well*, memorizing it instead of generalizing, and performing poorly on new data.\\n\\n**In Conclusion**\\n\\nAI is a complex and rapidly evolving field. It involves using data, algorithms, and models to create machines that can perform tasks that typically require human intelligence.  Understanding the core concepts and techniques is essential for anyone who wants to work with or understand AI. It\\'s important to remember that AI is a tool, and like any tool, it can be used for good or bad. Ethical considerations are crucial in the development and deployment of AI systems.\\n'}], 'role': 'model'}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Replace with your actual API key\n",
        "api_key = \"<Enter Key>\"\n",
        "\n",
        "# API endpoint URL\n",
        "url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=\" + api_key\n",
        "\n",
        "# Request headers\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Request data (prompt)\n",
        "data = {\n",
        "    \"contents\": [\n",
        "        {\n",
        "            \"parts\": [\n",
        "                {\"text\": \"Explain how AI works\"}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "try:\n",
        "  # Send POST request to Gemini API\n",
        "  response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "  # Check for successful request\n",
        "  response.raise_for_status()\n",
        "\n",
        "  # Process the JSON response\n",
        "  response_json = response.json()\n",
        "\n",
        "  # Extract and print the generated text\n",
        "  generated_text = response_json.get('candidates', [{}])[0].get('content', '')\n",
        "  print(generated_text)\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "  print(f\"An error occurred: {e}\")\n",
        "except (KeyError, IndexError) as e:\n",
        "  print(f\"Error parsing response: {e}\")\n",
        "  print(f\"Full response: {response.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xhzC6BVcAWru",
        "outputId": "69071530-b001-4cdf-ac76-fcb1785f3f0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.5)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
            "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.160.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.25.6)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.10.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.68.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.27.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
            "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
            "Installing collected packages: google-ai-generativelanguage\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.16\n",
            "    Uninstalling google-ai-generativelanguage-0.6.16:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.16\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 2.0.11 requires google-ai-generativelanguage<0.7.0,>=0.6.16, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-ai-generativelanguage-0.6.15\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "1b804315a6a04bf19ee458f8ae46e091",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.37)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.0.11)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain-google-genai)\n",
            "  Using cached google_ai_generativelanguage-0.6.16-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.37 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.37)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.10.6)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.25.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.3.9)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.27.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.68.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.3.1)\n",
            "Using cached google_ai_generativelanguage-0.6.16-py3-none-any.whl (1.4 MB)\n",
            "Installing collected packages: google-ai-generativelanguage\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.16 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-ai-generativelanguage-0.6.16\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "0d575db15b9146cd9f7364f872310a5f",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.37 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.37)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.19)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.9)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain-community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.37->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain-community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber\n",
        "!pip install google-generativeai\n",
        "!pip install langchain\n",
        "!pip install faiss-cpu\n",
        "!pip install -U langchain-google-genai\n",
        "!pip install -U langchain-community\n",
        "\n",
        "import pdfplumber\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings # Import GoogleGenerativeAIEmbeddings from langchain_google_genai\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMhSYZVr-TsS"
      },
      "source": [
        "### **Installing Dependencies:**\n",
        "pdfplumber: Extracts text from PDF files.\n",
        "google-generativeai: Provides access to Google's Generative AI models.\n",
        "langchain: A framework for working with LLMs (large language models).\n",
        "faiss-cpu: A library for efficient similarity search and clustering.\n",
        "langchain-google-genai: Integration of LangChain with Google Generative AI.\n",
        "langchain-community: An updated community-driven LangChain package.\n",
        "Importing Required Libraries:\n",
        "\n",
        "**pdfplumber:** Reads and extracts text from PDF documents.\n",
        "**google.generativeai:** Connects to Google's Generative AI models.\n",
        "**GoogleGenerativeAIEmbeddings:** Generates vector embeddings from text using Google's AI.\n",
        "**FAISS:** A vector store for storing and retrieving similar text chunks.\n",
        "**TextLoader:** Handles loading text-based documents (not used in this script).\n",
        "**RecursiveCharacterTextSplitter:** Splits text into smaller chunks for better processing.\n",
        "**Document: **Represents a structured text document.\n",
        "**GoogleGenerativeAI:** Utilizes Google's AI model for generating responses.\n",
        "**os:** Used for setting environment variables (like API keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGawgyjM-17a"
      },
      "outputs": [],
      "source": [
        "# Set up Gemini API Key (replace with your actual API key)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"<ENTER KEY>\"\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a given PDF file using pdfplumber.\"\"\"\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\" if page.extract_text() else \"\"  # Avoid None values\n",
        "    return text.strip()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1bqEhh5A0f1"
      },
      "source": [
        "# **Key Functionalities**\n",
        "# **1. API Key Configuration**\n",
        "The Google Gemini API Key is set as an environment variable using os.environ[\"GOOGLE_API_KEY\"].\n",
        "This allows secure access to Google's Generative AI models for embedding generation and text-based responses.\n",
        "# **2. Extracting Text from PDFs**(extract_text_from_pdf)\n",
        "Uses pdfplumber to read and extract text from a given PDF file.\n",
        "Iterates through each page of the document and retrieves its text.\n",
        "Handles None values to prevent errors during processing.\n",
        "Returns the cleaned text as a single string.\n",
        "# 3.**Processing and Vectorizing** the Extracted Text (vectorize_pdf)\n",
        "Calls extract_text_from_pdf(pdf_path) to retrieve the text content of a PDF.\n",
        "Prepares the extracted text for further processing and vector embedding (though vectorization is not yet implemented in this snippet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m82yHHf-BRKW"
      },
      "outputs": [],
      "source": [
        "def vectorize_pdf(pdf_path):\n",
        "    \"\"\"Processes and vectorizes the text from a PDF file.\"\"\"\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Split text into smaller chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks = text_splitter.split_text(text)\n",
        "\n",
        "    # Convert chunks into Document objects\n",
        "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
        "\n",
        "    # Initialize Google Gemini Embeddings\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "    # Store in FAISS vector database\n",
        "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
        "    return vectorstore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs3XXT_WBkJE"
      },
      "source": [
        "# **Key Components**\n",
        "# **1. Splitting Text into Chunks**\n",
        "\n",
        "Why split the text? Large documents can be difficult to process, so breaking them into smaller segments ensures better search accuracy.\n",
        "RecursiveCharacterTextSplitter:\n",
        "chunk_size=500: Each text chunk will have approximately 500 characters.\n",
        "chunk_overlap=50: Ensures some overlap between consecutive chunks to maintain context.\n",
        "# 2. **Converting Chunks into Document Objects**\n",
        "\n",
        "Each chunk is wrapped inside a Document object, making it compatible with vectorization and retrieval models.\n",
        "# 3. **Initializing Google Gemini Embeddings**\n",
        "\n",
        "GoogleGenerativeAIEmbeddings converts text chunks into numerical vector representations.\n",
        "These vectors allow the system to perform similarity searches and retrieve relevant content.\n",
        "# 4. **Storing Vectors in FAISS**\n",
        "\n",
        "FAISS (Facebook AI Similarity Search) is an efficient indexing system for fast similarity searches.\n",
        "It stores document vectors, enabling quick and accurate retrieval when querying the document later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVFfX0RxBl3U"
      },
      "outputs": [],
      "source": [
        "def query_pdf(vectorstore, query):\n",
        "    \"\"\"Retrieves relevant information from the vectorstore and generates a response.\"\"\"\n",
        "    # Search for relevant documents\n",
        "    docs = vectorstore.similarity_search(query, k=3)\n",
        "    context = \"\\n\".join([doc.page_content for doc in docs]\n",
        "                        )\n",
        "\n",
        "    # Initialize Gemini Flash 2.0 Lite model\n",
        "    llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n",
        "    # Generate response based on context\n",
        "    prompt = f\"Using the following extracted information from a PDF, answer the user's question:\\n\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "    response = llm.invoke(prompt)\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DynlYMcCfSc"
      },
      "source": [
        "# **1. Retrieving Relevant Information**\n",
        "\n",
        "similarity_search(query, k=3): Searches the FAISS database for the top 3 most relevant text chunks related to the users query.\n",
        "Joins retrieved document chunks into a single string (context) to provide meaningful context for the LLM.\n",
        "# **2. Initializing the Google Gemini Model**\n",
        "\n",
        "Loads the Gemini 1.5 Flash model, a fast and efficient generative AI designed for real-time question answering.\n",
        "# **3. Creating the Prompt for AI Response Generation**\n",
        "\n",
        "Prompt Engineering:\n",
        "Provides retrieved context from the PDF.\n",
        "Clearly defines the users question to guide the AI model.\n",
        "Ensures the model stays factually grounded in the document content.\n",
        "# **4. Generating and Returning the Response**\n",
        "\n",
        "llm.invoke(prompt): Uses the AI model to generate an answer based on the context.\n",
        "Returns the AI-generated response to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b3a68KJCztT",
        "outputId": "ce3d2bac-66ef-4120-f3a8-f6570321b102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ask a question (or type 'exit' to quit): are there any grammar errors\n",
            "\n",
            "Response: Yes, there are a few grammatical errors in the provided text.  Specifically:\n",
            "\n",
            "* **\"a more readable, functional and visually appealing system which capable of converting...\"** should be \"...a more readable, functional, and visually appealing system *that is* capable of converting...\"  The relative pronoun \"which\" is incorrectly used; \"that\" is needed here or a restructuring of the sentence.\n",
            "\n",
            "* **\"(movies.db) database\"** appears twice in succession.  This is redundant.\n",
            "\n",
            "\n",
            "The other points are understandable, even if not perfectly grammatically pristine.  The informality of the progress report likely accounts for these minor issues.\n",
            "\n",
            "Ask a question (or type 'exit' to quit): quit\n",
            "\n",
            "Response: The provided text is a progress report, not a conversation.  There is no information about quitting anything.  Therefore, there is no answer to the question \"quit\".\n",
            "\n",
            "Ask a question (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "pdf_path = \"/content/weekly-report-7.pdf\"  # Provide the path to your PDF file\n",
        "vectorstore = vectorize_pdf(pdf_path)\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nAsk a question (or type 'exit' to quit): \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    answer = query_pdf(vectorstore, query)\n",
        "    print(\"\\nResponse:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxYt86D8DFob"
      },
      "source": [
        "# **Key Components**\n",
        "# **1. Defining the PDF Path and Vectorizing Its content**\n",
        "\n",
        "pdf_path: Specifies the location of the PDF file to be processed.\n",
        "vectorize_pdf(pdf_path): Extracts, chunks, and embeds the PDF text into the FAISS vector database for efficient retrieval.\n",
        "# **2. User Input Loop for Querying the PDF**\n",
        "\n",
        "Starts an infinite loop to continuously accept user questions.\n",
        "Allows users to type queries dynamically.\n",
        "Includes an exit condition to terminate the program when \"exit\" is entered.\n",
        "# **3. Querying the Vector Database and Generating Responses**\n",
        "\n",
        "Checks if the user input is \"exit\" (case insensitive) and breaks the loop if true.\n",
        "Calls query_pdf(vectorstore, query) to retrieve the most relevant text chunks and generate a response using the AI model.\n",
        "Prints the AI-generated response for the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWm9iClmDb-8"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
